{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b587f4e6",
   "metadata": {},
   "source": [
    "# Filipino Fake News Detector\n",
    "\n",
    "This project was made by:\n",
    "- Justin Clyde Frongoso\n",
    "- Medwin Devilleres\n",
    "- Rae Gabriel Samonte\n",
    "- Alquen Antonio Sarmiento\n",
    "\n",
    "This project is implemented as a chrome extension tool that helps identify if an article contains fake content in the form of a paragraph, phrase or sentence through the use of the Multinomial Naive Bayes model in predicting the validity of Filipino news articles. This Jupyter notebook is made for documentation and demonstration only.\n",
    "\n",
    "The steps for the implementation are given below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf414dc",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We first import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb67bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from stops import stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a3053",
   "metadata": {},
   "source": [
    "## 2. Import Dataset\n",
    "\n",
    "We now import the dataset as well as separate the features (in this case, only the article) and the label. The label 0 corresponds to legitimate articles and the label 1 corresponds to fake articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d155445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ayon sa TheWrap.com, naghain ng kaso si Krupa,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Kilala rin ang singer sa pagkumpas ng kanyang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>BLANTYRE, Malawi (AP) -- Bumiyahe patungong Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Kasama sa programa ang pananalangin, bulaklak ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Linisin ang Friendship Department dahil dadala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            article\n",
       "0      0  Ayon sa TheWrap.com, naghain ng kaso si Krupa,...\n",
       "1      0  Kilala rin ang singer sa pagkumpas ng kanyang ...\n",
       "2      0  BLANTYRE, Malawi (AP) -- Bumiyahe patungong Ma...\n",
       "3      0  Kasama sa programa ang pananalangin, bulaklak ...\n",
       "4      0  Linisin ang Friendship Department dahil dadala..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'full.csv'\n",
    "data = pd.read_csv(path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb25f04a",
   "metadata": {},
   "source": [
    "We check the split of fake and legitimate articles from the dataset through the use of `value_counts()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205aa15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1598\n",
       "1    1598\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = data.label.value_counts()\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88387003",
   "metadata": {},
   "source": [
    "We set the key values for the feature `article` and `label` to `X` and `y`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9507be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['article']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b98b0e",
   "metadata": {},
   "source": [
    "ano to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846fb746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Article Length: 365.94180225281605\n"
     ]
    }
   ],
   "source": [
    "lens = 0\n",
    "for i in X:\n",
    "    \n",
    "    cur_len = len(i.split())\n",
    "    lens += cur_len\n",
    "\n",
    "print(f\"Average Article Length: {lens / count[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176368b",
   "metadata": {},
   "source": [
    "## 3. Splitting the Dataset (for training and testing)\n",
    "\n",
    "The data will now be splitted into two sets: the training and test set. Since there are only 3000+ rows, we are splitting the data in a 80:20 split for the training and test set, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48082ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2556,)\n",
      "(640,)\n",
      "(2556,)\n",
      "(640,)\n"
     ]
    }
   ],
   "source": [
    "## X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99a1c7",
   "metadata": {},
   "source": [
    "## 4. Vectorizing the Dataset\n",
    "\n",
    "We vectorize the dataset into numerical categories in order to easily categorize and fit them using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "216ebee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<640x34427 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 52560 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words = [word for word in stop_words])\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fa231",
   "metadata": {},
   "source": [
    "## 5. Building the Model\n",
    "\n",
    "We are using the Multinomial Naive Bayes Classifier as it is suitable for classification of discrete features.  It is also computationally efficient and is commonly used for labeling articles and text classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b10c450e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d15298d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa413909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9203125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484e8b9",
   "metadata": {},
   "source": [
    "## 6. Predicting Text\n",
    "\n",
    "Now that we trained the model, it is now time to use it and predict some pieces of text. To begin, input any article and see whether it is trustworthy or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aba9add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def manual_predict(model, vectorizer, text):\n",
    "    inp_arr = []\n",
    "    inp_arr.append(text)\n",
    "    inp_dtm = vectorizer.transform(inp_arr)\n",
    "    res = model.predict(inp_dtm)[0]\n",
    "    return res\n",
    "\n",
    "inp = str(input())\n",
    "print(manual_predict(nb, vect, inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df217c",
   "metadata": {},
   "source": [
    "## 7. Exporting the Model\n",
    "We export the model to a binary file that can be imported by the API to be used for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ba30ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model_pickle', 'wb') as f:\n",
    "    pickle.dump(nb, f)\n",
    "with open('vect_pickle', 'wb') as f:\n",
    "    pickle.dump(vect, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "095898ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_pickle', 'rb') as f:\n",
    "    imported_model = pickle.load (f)\n",
    "with open('vect_pickle', 'rb') as f:\n",
    "    imported_vect = pickle.load (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9751c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "inp = str(input())\n",
    "print(manual_predict(imported_model, imported_vect, inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a56f58",
   "metadata": {},
   "source": [
    "## 8. Additional Features\n",
    "In this section we introduce additional features for the extension. We will use two more models, the SVM model and Logistic Regression, to identify the likelihood that an article is trustworthy or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1378548",
   "metadata": {},
   "source": [
    "### i. SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79928e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and numerical vectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9526ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data into numeric categorizations\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_features = vectorizer.fit_transform(X_train)\n",
    "test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46a98cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93       323\n",
      "           1       0.94      0.91      0.93       317\n",
      "\n",
      "    accuracy                           0.93       640\n",
      "   macro avg       0.93      0.93      0.93       640\n",
      "weighted avg       0.93      0.93      0.93       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and test\n",
    "classifier = svm.SVC()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08144897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "import pickle\n",
    "with open('model_pickle_svm', 'wb') as f:\n",
    "    pickle.dump(nb, f)\n",
    "with open('vect_pickle_svm', 'wb') as f:\n",
    "    pickle.dump(vect, f)\n",
    "with open('model_pickle_svm', 'rb') as f:\n",
    "    imported_model_svm = pickle.load (f)\n",
    "with open('vect_pickle_svm', 'rb') as f:\n",
    "    imported_vect_svm = pickle.load (f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155a0eb",
   "metadata": {},
   "source": [
    "### ii. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1b8da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and numerical vectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f43ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data into numeric categorizations\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_features = vectorizer.fit_transform(X_train)\n",
    "test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82f510e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       323\n",
      "           1       0.91      0.91      0.91       317\n",
      "\n",
      "    accuracy                           0.91       640\n",
      "   macro avg       0.91      0.91      0.91       640\n",
      "weighted avg       0.91      0.91      0.91       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and test\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6fae85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "import pickle\n",
    "with open('model_pickle_logistic_regression', 'wb') as f:\n",
    "    pickle.dump(nb, f)\n",
    "with open('vect_pickle_logistic_regression', 'wb') as f:\n",
    "    pickle.dump(vect, f)\n",
    "with open('model_pickle_logistic_regression', 'rb') as f:\n",
    "    imported_model_logistic_regression = pickle.load (f)\n",
    "with open('vect_pickle_logistic_regression', 'rb') as f:\n",
    "    imported_vect_logistic_regression = pickle.load (f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
