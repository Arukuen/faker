{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b587f4e6",
   "metadata": {},
   "source": [
    "# Filipino Fake News Detector\n",
    "\n",
    "This project was made by:\n",
    "- Justin Clyde Frongoso\n",
    "- Medwin Devilleres\n",
    "- Rae Gabriel Samonte\n",
    "- Alquen Antonio Sarmiento\n",
    "\n",
    "This project is implemented as a chrome extension tool that helps identify if an article contains fake content in the form of a paragraph, phrase or sentence through the use of the Multinomial Naive Bayes model in predicting the validity of Filipino news articles. This Jupyter notebook is made for documentation and demonstration only.\n",
    "\n",
    "The steps for the implementation can be seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf414dc",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb67bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from stops import stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a3053",
   "metadata": {},
   "source": [
    "## 2. Import Dataset\n",
    "\n",
    "We now import the dataset as well as separate the features (in this case, only the article) and the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d155445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ayon sa TheWrap.com, naghain ng kaso si Krupa,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Kilala rin ang singer sa pagkumpas ng kanyang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>BLANTYRE, Malawi (AP) -- Bumiyahe patungong Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Kasama sa programa ang pananalangin, bulaklak ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Linisin ang Friendship Department dahil dadala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            article\n",
       "0      0  Ayon sa TheWrap.com, naghain ng kaso si Krupa,...\n",
       "1      0  Kilala rin ang singer sa pagkumpas ng kanyang ...\n",
       "2      0  BLANTYRE, Malawi (AP) -- Bumiyahe patungong Ma...\n",
       "3      0  Kasama sa programa ang pananalangin, bulaklak ...\n",
       "4      0  Linisin ang Friendship Department dahil dadala..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'full.csv'\n",
    "data = pd.read_csv(path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205aa15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1598\n",
       "1    1598\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9507be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['article']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "846fb746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Article Length: 364.8003742981909\n"
     ]
    }
   ],
   "source": [
    "lens = 0\n",
    "for i in X:\n",
    "    cur_len = len(i.split())\n",
    "    lens += cur_len\n",
    "\n",
    "print(f\"Average Article Length: {lens / 1603}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176368b",
   "metadata": {},
   "source": [
    "## 3. Splitting the Dataset (for training and testing)\n",
    "\n",
    "The data will now be splitted into two: training set and test set. Since there are only 3000+ rows, we are splitting the data in this way: 80% training and 20% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d713fc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2556,)\n",
      "(640,)\n",
      "(2556,)\n",
      "(640,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99a1c7",
   "metadata": {},
   "source": [
    "## 4. Vectorizing the Dataset\n",
    "\n",
    "Now, we need to vectorize the dataset to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "216ebee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<640x34908 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 50720 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words = [word for word in stop_words])\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fa231",
   "metadata": {},
   "source": [
    "## 5. Building the Model\n",
    "\n",
    "We are using the Multinomial Naive Bayes Classifier as it is suitable for classification of discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10c450e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d15298d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa413909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9078125"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484e8b9",
   "metadata": {},
   "source": [
    "## 6. Predicting Text\n",
    "\n",
    "Now that we trained the model, it is now time to use it and predict some pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aba9add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def manual_predict(model, vectorizer, text):\n",
    "    inp_arr = []\n",
    "    inp_arr.append(text)\n",
    "    inp_dtm = vectorizer.transform(inp_arr)\n",
    "    res = model.predict(inp_dtm)[0]\n",
    "    return res\n",
    "\n",
    "inp = str(input())\n",
    "print(manual_predict(nb, vect, inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df217c",
   "metadata": {},
   "source": [
    "## 7. Exporting the Model\n",
    "We export the model to a binary file that can be imported by the API to be used for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba30ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model_pickle', 'wb') as f:\n",
    "    pickle.dump(nb, f)\n",
    "with open('vect_pickle', 'wb') as f:\n",
    "    pickle.dump(vect, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "095898ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_pickle', 'rb') as f:\n",
    "    imported_model = pickle.load (f)\n",
    "with open('vect_pickle', 'rb') as f:\n",
    "    imported_vect = pickle.load (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9751c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "inp = str(input())\n",
    "print(manual_predict(imported_model, imported_vect, inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a56f58",
   "metadata": {},
   "source": [
    "## 8. Additional Features\n",
    "In this section we introduce additional features for the extension. We will two more models to identify the likelihood that an article is trustworthy or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1378548",
   "metadata": {},
   "source": [
    "### i. SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79928e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and numerical vectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9526ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data into numeric categorizations\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_features = vectorizer.fit_transform(X_train)\n",
    "test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a98cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and test\n",
    "classifier = svm.SVC()()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155a0eb",
   "metadata": {},
   "source": [
    "### ii. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1b8da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and numerical vectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f43ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data into numeric categorizations\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_features = vectorizer.fit_transform(X_train)\n",
    "test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82f510e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92       304\n",
      "           1       0.93      0.92      0.93       336\n",
      "\n",
      "    accuracy                           0.92       640\n",
      "   macro avg       0.92      0.92      0.92       640\n",
      "weighted avg       0.92      0.92      0.92       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and test\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_features, y_train)\n",
    "predictions = classifier.predict(test_features)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fae85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
